{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3131eff-0568-4f22-aa13-75a1c1db3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f02f14b4-a210-4301-9f3b-fd31e7e94bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>Sampling_Date</th>\n",
       "      <th>State</th>\n",
       "      <th>Species</th>\n",
       "      <th>Pre_GSHH_NDVI</th>\n",
       "      <th>Height_Ave_cm</th>\n",
       "      <th>target_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1011485656__Dry_Clover_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Clover_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1011485656__Dry_Dead_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Dead_g</td>\n",
       "      <td>31.9984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1011485656__Dry_Green_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Green_g</td>\n",
       "      <td>16.2751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1011485656__Dry_Total_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Total_g</td>\n",
       "      <td>48.2735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1011485656__GDM_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>GDM_g</td>\n",
       "      <td>16.2750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id              image_path Sampling_Date State  \\\n",
       "0  ID1011485656__Dry_Clover_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "1    ID1011485656__Dry_Dead_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "2   ID1011485656__Dry_Green_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "3   ID1011485656__Dry_Total_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "4         ID1011485656__GDM_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "\n",
       "           Species  Pre_GSHH_NDVI  Height_Ave_cm   target_name   target  \n",
       "0  Ryegrass_Clover           0.62         4.6667  Dry_Clover_g   0.0000  \n",
       "1  Ryegrass_Clover           0.62         4.6667    Dry_Dead_g  31.9984  \n",
       "2  Ryegrass_Clover           0.62         4.6667   Dry_Green_g  16.2751  \n",
       "3  Ryegrass_Clover           0.62         4.6667   Dry_Total_g  48.2735  \n",
       "4  Ryegrass_Clover           0.62         4.6667         GDM_g  16.2750  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fde8d30-608e-408a-b09c-7e56274813e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract train image features: 100%|██████████████████████████████████████████████████| 357/357 [01:01<00:00,  5.83it/s]\n",
      "Extract test image features: 100%|███████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows (images): 357 features: 20\n",
      "Test rows (images): 1 features: 20\n",
      "LightGBM not available, falling back to RandomForest. (Import error: No module named 'lightgbm')\n",
      "Training MultiOutput RandomForest...\n",
      "\n",
      "Per-target R² on validation:\n",
      "  Dry_Green_g: 0.9981\n",
      "  Dry_Dead_g: 0.9952\n",
      "  Dry_Clover_g: 0.9952\n",
      "  GDM_g: 0.9990\n",
      "  Dry_Total_g: 0.9993\n",
      "\n",
      "Weighted R² on validation: 0.999150\n",
      "Predicting test set...\n",
      "Wrote submission.csv with 5 rows. First 6 rows:\n",
      "                 sample_id   target\n",
      " ID1001187975__Dry_Green_g 0.035444\n",
      "  ID1001187975__Dry_Dead_g 0.000000\n",
      "ID1001187975__Dry_Clover_g 0.000500\n",
      "       ID1001187975__GDM_g 2.372839\n",
      " ID1001187975__Dry_Total_g 4.313450\n"
     ]
    }
   ],
   "source": [
    "# AI generated code, just for testing:\n",
    "# Full functioning baseline pipeline — single cell\n",
    "# Requirements: pandas, numpy, pillow, scikit-learn, tqdm\n",
    "# Optional: lightgbm (preferred). If not present, RandomForest will be used.\n",
    "# Place train.csv and test.csv in the working dir. Image paths will be taken from CSV if available,\n",
    "# otherwise the cell will try IMAGE_DIR/{image_id}.jpg\n",
    "\n",
    "import os, sys, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# -------- USER SETTINGS (edit if needed) --------\n",
    "IMAGE_DIR = \"images\"          # fallback dir (tries CSV image_path first)\n",
    "IMG_RESIZE = (400, 200)       # downscale for speed (width, height)\n",
    "RANDOM_SEED = 42\n",
    "USE_LIGHTGBM = True           # try to use LightGBM; will auto-fallback if not installed\n",
    "LGBM_PARAMS = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,\n",
    "    \"min_child_samples\": 20,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": RANDOM_SEED,\n",
    "}\n",
    "RF_PARAMS = {\"n_estimators\": 200, \"n_jobs\": -1, \"random_state\": RANDOM_SEED}\n",
    "# ------------------------------------------------\n",
    "\n",
    "# load csvs\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# derive image_id robustly\n",
    "def derive_image_id_from_row(row):\n",
    "    # sample_id like ID123__Dry_Green_g\n",
    "    if \"sample_id\" in row and isinstance(row[\"sample_id\"], str) and \"__\" in row[\"sample_id\"]:\n",
    "        return row[\"sample_id\"].split(\"__\")[0]\n",
    "    # image_path like \"train/ID123.jpg\"\n",
    "    if \"image_path\" in row and isinstance(row[\"image_path\"], str) and len(str(row[\"image_path\"]).strip())>0:\n",
    "        return os.path.splitext(os.path.basename(row[\"image_path\"]))[0]\n",
    "    # fallback to index string\n",
    "    return str(row.name)\n",
    "\n",
    "train = train.copy()\n",
    "test  = test.copy()\n",
    "train[\"image_id\"] = train.apply(derive_image_id_from_row, axis=1)\n",
    "test[\"image_id\"]  = test.apply(derive_image_id_from_row, axis=1)\n",
    "\n",
    "# Validate expected long-format columns in train\n",
    "if not {\"target_name\", \"target\"}.issubset(set(train.columns)):\n",
    "    raise ValueError(\"train.csv must contain 'target_name' and 'target' columns (one row per image+target).\")\n",
    "\n",
    "targets = [\"Dry_Green_g\",\"Dry_Dead_g\",\"Dry_Clover_g\",\"GDM_g\",\"Dry_Total_g\"]\n",
    "\n",
    "# pivot train to wide: one row per image_id with 5 target columns\n",
    "train_wide = train.pivot_table(index=\"image_id\", columns=\"target_name\", values=\"target\", aggfunc=\"first\").reset_index()\n",
    "# ensure all target columns exist\n",
    "for t in targets:\n",
    "    if t not in train_wide.columns:\n",
    "        train_wide[t] = np.nan\n",
    "\n",
    "# collect a single metadata row per image (first occurrence)\n",
    "meta_columns = [c for c in [\"image_path\",\"Pre_GSHH_NDVI\",\"Height_Ave_cm\",\"Sampling_Date\",\"State\",\"Species\"] if c in train.columns]\n",
    "meta = train.drop_duplicates(subset=[\"image_id\"]).set_index(\"image_id\")[meta_columns].reset_index()\n",
    "train_wide = train_wide.merge(meta, on=\"image_id\", how=\"left\")\n",
    "\n",
    "# For test, build test_images with unique image_id + image_path if present\n",
    "test_meta_cols = [c for c in [\"image_path\",\"Pre_GSHH_NDVI\",\"Height_Ave_cm\",\"Sampling_Date\",\"State\",\"Species\"] if c in test.columns]\n",
    "if len(test_meta_cols) > 0:\n",
    "    test_images = test.drop_duplicates(subset=[\"image_id\"]).set_index(\"image_id\")[test_meta_cols].reset_index()\n",
    "else:\n",
    "    test_images = pd.DataFrame({\"image_id\": test[\"image_id\"].unique()})\n",
    "\n",
    "# Build path_map (image_id -> image_path) from available CSV rows\n",
    "path_map = {}\n",
    "for df in (train, test):\n",
    "    if \"image_path\" in df.columns:\n",
    "        for _, r in df.iterrows():\n",
    "            imgid = r[\"image_id\"]\n",
    "            p = r[\"image_path\"]\n",
    "            if pd.isna(p) or str(p).strip()==\"\":\n",
    "                continue\n",
    "            path_map[imgid] = str(p)\n",
    "\n",
    "# --- Image feature extractor ---\n",
    "def extract_image_features_by_path_or_id(img_id, resize=IMG_RESIZE, image_dir=IMAGE_DIR):\n",
    "    # candidates to try\n",
    "    candidates = []\n",
    "    if img_id in path_map:\n",
    "        candidates.append(path_map[img_id])\n",
    "        candidates.append(os.path.basename(path_map[img_id]))\n",
    "    candidates.append(os.path.join(image_dir, f\"{img_id}.jpg\"))\n",
    "    candidates.append(os.path.join(image_dir, f\"{img_id}.JPG\"))\n",
    "    candidates.append(f\"{img_id}.jpg\")\n",
    "    candidates.append(f\"{img_id}.JPG\")\n",
    "    img = None\n",
    "    used_path = None\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            img = Image.open(c).convert(\"RGB\")\n",
    "            used_path = c\n",
    "            break\n",
    "        except Exception:\n",
    "            img = None\n",
    "    if img is None:\n",
    "        # fallback zeros\n",
    "        return {\n",
    "            \"r_mean\": 0.0, \"g_mean\": 0.0, \"b_mean\": 0.0,\n",
    "            \"r_std\": 0.0, \"g_std\": 0.0, \"b_std\": 0.0,\n",
    "            \"exg\": 0.0, \"brightness\": 0.0, \"g_ratio\": 0.0, \"p90_g\": 0.0,\n",
    "            \"img_found\": 0\n",
    "        }\n",
    "    img = img.resize(resize)\n",
    "    arr = np.asarray(img).astype(np.float32)\n",
    "    r = arr[...,0]; g = arr[...,1]; b = arr[...,2]\n",
    "    r_mean = float(r.mean()); g_mean = float(g.mean()); b_mean = float(b.mean())\n",
    "    r_std = float(r.std()); g_std = float(g.std()); b_std = float(b.std())\n",
    "    exg = float((2.0*g - r - b).mean())\n",
    "    brightness = float(arr.mean())\n",
    "    denom = (r_mean + b_mean)/2.0 + 1e-6\n",
    "    g_ratio = float(g_mean / denom)\n",
    "    p90_g = float(np.percentile(g, 90))\n",
    "    return {\n",
    "        \"r_mean\": r_mean, \"g_mean\": g_mean, \"b_mean\": b_mean,\n",
    "        \"r_std\": r_std, \"g_std\": g_std, \"b_std\": b_std,\n",
    "        \"exg\": exg, \"brightness\": brightness, \"g_ratio\": g_ratio, \"p90_g\": p90_g,\n",
    "        \"img_found\": 1\n",
    "    }\n",
    "\n",
    "# compute image features for train_wide images\n",
    "train_ids = train_wide[\"image_id\"].tolist()\n",
    "train_img_feats = []\n",
    "for iid in tqdm(train_ids, desc=\"Extract train image features\"):\n",
    "    f = extract_image_features_by_path_or_id(iid)\n",
    "    f[\"image_id\"] = iid\n",
    "    train_img_feats.append(f)\n",
    "train_img_feats = pd.DataFrame(train_img_feats)\n",
    "\n",
    "# compute image features for test images\n",
    "test_ids = test_images[\"image_id\"].tolist()\n",
    "test_img_feats = []\n",
    "for iid in tqdm(test_ids, desc=\"Extract test image features\"):\n",
    "    f = extract_image_features_by_path_or_id(iid)\n",
    "    f[\"image_id\"] = iid\n",
    "    test_img_feats.append(f)\n",
    "test_img_feats = pd.DataFrame(test_img_feats)\n",
    "\n",
    "# merge image features into train_wide and test_images\n",
    "train_final = train_wide.merge(train_img_feats, on=\"image_id\", how=\"left\")\n",
    "test_final  = test_images.merge(test_img_feats, on=\"image_id\", how=\"left\")\n",
    "\n",
    "# --- Build feature table ---\n",
    "# Use numeric columns + selected metadata (Pre_GSHH_NDVI, Height_Ave_cm) + label-encoded State/Species\n",
    "def prepare_features(df, fit_encoders=None):\n",
    "    df2 = df.copy()\n",
    "    # numeric features from dataframe\n",
    "    numeric = df2.select_dtypes(include=[np.number]).copy()\n",
    "    # fill NaNs\n",
    "    numeric = numeric.fillna(0.0)\n",
    "    # label encode State and Species\n",
    "    encoders = {} if fit_encoders is None else fit_encoders\n",
    "    for col in [\"State\",\"Species\"]:\n",
    "        if col in df2.columns:\n",
    "            if fit_encoders is None:\n",
    "                le = LabelEncoder()\n",
    "                numeric[col+\"_le\"] = le.fit_transform(df2[col].fillna(\"NA\").astype(str))\n",
    "                encoders[col] = le\n",
    "            else:\n",
    "                le = encoders.get(col, None)\n",
    "                if le is not None:\n",
    "                    numeric[col+\"_le\"] = le.transform(df2[col].fillna(\"NA\").astype(str))\n",
    "                else:\n",
    "                    numeric[col+\"_le\"] = 0\n",
    "    return numeric, encoders\n",
    "\n",
    "X_all, encs = prepare_features(train_final, fit_encoders=None)\n",
    "X_test_all, _ = prepare_features(test_final, fit_encoders=encs)\n",
    "\n",
    "# Ensure X_test has same columns in same order\n",
    "for c in X_all.columns:\n",
    "    if c not in X_test_all.columns:\n",
    "        X_test_all[c] = 0.0\n",
    "X_test_all = X_test_all[X_all.columns]\n",
    "\n",
    "y = train_final[targets].copy()\n",
    "\n",
    "print(\"Train rows (images):\", X_all.shape[0], \"features:\", X_all.shape[1])\n",
    "print(\"Test rows (images):\", X_test_all.shape[0], \"features:\", X_test_all.shape[1])\n",
    "\n",
    "# train/validation split (by image)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_all, y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "\n",
    "# Try to import LightGBM\n",
    "use_lgbm = False\n",
    "if USE_LIGHTGBM:\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        from lightgbm import LGBMRegressor\n",
    "        use_lgbm = True\n",
    "        print(\"LightGBM available — using LGBMRegressor per target.\")\n",
    "    except Exception as e:\n",
    "        print(\"LightGBM not available, falling back to RandomForest. (Import error: {})\".format(e))\n",
    "\n",
    "# Build per-target regressors\n",
    "models = {}\n",
    "if use_lgbm:\n",
    "    # train a separate LGBM for each target (simple, without early stopping to keep runtime bounded)\n",
    "    for t in targets:\n",
    "        print(f\"Training LGBM for target: {t}\")\n",
    "        m = LGBMRegressor(**LGBM_PARAMS)\n",
    "        m.fit(X_tr, y_tr[t], eval_set=[(X_val, y_val[t])], verbose=100)\n",
    "        models[t] = m\n",
    "else:\n",
    "    # MultiOutput RandomForest fallback\n",
    "    base = RandomForestRegressor(**RF_PARAMS)\n",
    "    mor = MultiOutputRegressor(base)\n",
    "    print(\"Training MultiOutput RandomForest...\")\n",
    "    mor.fit(X_tr, y_tr)\n",
    "    for i,t in enumerate(targets):\n",
    "        models[t] = mor.estimators_[i]  # estimator per target\n",
    "\n",
    "# Predict on validation\n",
    "y_val_pred = pd.DataFrame({t: models[t].predict(X_val) for t in targets}, index=y_val.index)\n",
    "\n",
    "# Print per-target R^2\n",
    "print(\"\\nPer-target R² on validation:\")\n",
    "for t in targets:\n",
    "    r2 = r2_score(y_val[t], y_val_pred[t])\n",
    "    print(f\"  {t}: {r2:.4f}\")\n",
    "\n",
    "# Weighted R^2 (competition weights)\n",
    "weights = {\"Dry_Green_g\":0.1,\"Dry_Dead_g\":0.1,\"Dry_Clover_g\":0.1,\"GDM_g\":0.2,\"Dry_Total_g\":0.5}\n",
    "def weighted_r2(y_true_df, y_pred_df, weights_map):\n",
    "    arr_true = []\n",
    "    arr_pred = []\n",
    "    arr_w = []\n",
    "    for t in y_true_df.columns:\n",
    "        arr_true.append(y_true_df[t].values)\n",
    "        arr_pred.append(y_pred_df[t].values)\n",
    "        arr_w.append(np.full_like(y_true_df[t].values, fill_value=weights_map.get(t,1.0), dtype=float))\n",
    "    y_true_flat = np.concatenate(arr_true)\n",
    "    y_pred_flat = np.concatenate(arr_pred)\n",
    "    w_flat = np.concatenate(arr_w)\n",
    "    mu = np.sum(w_flat * y_true_flat) / (np.sum(w_flat) + 1e-12)\n",
    "    ss_res = np.sum(w_flat * (y_true_flat - y_pred_flat)**2)\n",
    "    ss_tot = np.sum(w_flat * (y_true_flat - mu)**2)\n",
    "    return 1.0 - ss_res / (ss_tot + 1e-12)\n",
    "\n",
    "w_r2 = weighted_r2(y_val.reset_index(drop=True), y_val_pred.reset_index(drop=True), weights)\n",
    "print(f\"\\nWeighted R² on validation: {w_r2:.6f}\")\n",
    "\n",
    "# Predict on test set\n",
    "print(\"Predicting test set...\")\n",
    "test_preds = np.column_stack([models[t].predict(X_test_all) for t in targets])  # shape n_test x 5\n",
    "\n",
    "# Build long-format submission: sample_id = \"{image_id}__{target}\"\n",
    "sub_rows = []\n",
    "for i, img_id in enumerate(test_ids):\n",
    "    for j, t in enumerate(targets):\n",
    "        sample_id = f\"{img_id}__{t}\"\n",
    "        val = float(test_preds[i, j])\n",
    "        # Ensure non-negative prediction (biomass cannot be negative)\n",
    "        if np.isnan(val):\n",
    "            val = 0.0\n",
    "        if val < 0:\n",
    "            val = 0.0\n",
    "        sub_rows.append([sample_id, val])\n",
    "\n",
    "submission = pd.DataFrame(sub_rows, columns=[\"sample_id\", \"target\"])\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Wrote submission.csv with\", len(submission), \"rows. First 6 rows:\")\n",
    "print(submission.head(6).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333854bd-31ba-4090-a2e3-f48e03ff489d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
