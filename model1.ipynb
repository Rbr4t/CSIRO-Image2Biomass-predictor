{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9056968",
   "metadata": {},
   "source": [
    "If best_fusion_multi.pth is available, no need to train the NN again, just run cells 1-2 and 12-14 to get the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d9af8",
   "metadata": {},
   "source": [
    "1. Install and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95edcb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your environment already has these, you can skip installs.\n",
    "#!pip install torch torchvision pandas scikit-learn tqdm --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a0ac9e",
   "metadata": {},
   "source": [
    "2. Configuration (paths, target list, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81f8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Paths ---\n",
    "KAGGLE_INPUT_DIR = \"\" # \"/kaggle/input/csiro-biomass/\"\n",
    "KAGGLE_OUTPUT_DIR = \"\" #\"/kaggle/working/\"\n",
    "TRAIN_CSV = KAGGLE_INPUT_DIR+\"train.csv\"\n",
    "TEST_CSV = KAGGLE_INPUT_DIR+\"test.csv\"\n",
    "TRAIN_IMG_ROOT = pathlib.Path(KAGGLE_INPUT_DIR+\"train\")  # not prepended if CSV has full/relative paths; we'll handle both\n",
    "TEST_IMG_ROOT = pathlib.Path(KAGGLE_INPUT_DIR+\"test\")\n",
    "SAMPLE_SUBMISSION = KAGGLE_OUTPUT_DIR+\"sample_submission.csv\"  # optional template; if missing we'll build from test.csv\n",
    "\n",
    "# --- Targets (expected target_name values) ---\n",
    "TARGET_NAMES = [\n",
    "    \"Dry_Clover_g\",\n",
    "    \"Dry_Dead_g\",\n",
    "    \"Dry_Green_g\",\n",
    "    \"Dry_Total_g\",\n",
    "    \"GDM_g\"\n",
    "]\n",
    "\n",
    "# --- Training hyperparameters (tweak as needed) ---\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "EPOCHS = 1\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "WORKERS = 1\n",
    "PIN_MEMORY = False\n",
    "N_SPLITS = 5\n",
    "KFOLD_RANDOM_STATE = 42\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e056f41",
   "metadata": {},
   "source": [
    "3. Load & pivot train.csv into one row per sample (with tabular features + multi-target vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109fa281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>Sampling_Date</th>\n",
       "      <th>State</th>\n",
       "      <th>Species</th>\n",
       "      <th>Pre_GSHH_NDVI</th>\n",
       "      <th>Height_Ave_cm</th>\n",
       "      <th>target_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1011485656__Dry_Clover_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Clover_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1011485656__Dry_Dead_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Dead_g</td>\n",
       "      <td>31.9984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1011485656__Dry_Green_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Green_g</td>\n",
       "      <td>16.2751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1011485656__Dry_Total_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Total_g</td>\n",
       "      <td>48.2735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1011485656__GDM_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>GDM_g</td>\n",
       "      <td>16.2750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id              image_path Sampling_Date State  \\\n",
       "0  ID1011485656__Dry_Clover_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "1    ID1011485656__Dry_Dead_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "2   ID1011485656__Dry_Green_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "3   ID1011485656__Dry_Total_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "4         ID1011485656__GDM_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "\n",
       "           Species  Pre_GSHH_NDVI  Height_Ave_cm   target_name   target  \n",
       "0  Ryegrass_Clover           0.62         4.6667  Dry_Clover_g   0.0000  \n",
       "1  Ryegrass_Clover           0.62         4.6667    Dry_Dead_g  31.9984  \n",
       "2  Ryegrass_Clover           0.62         4.6667   Dry_Green_g  16.2751  \n",
       "3  Ryegrass_Clover           0.62         4.6667   Dry_Total_g  48.2735  \n",
       "4  Ryegrass_Clover           0.62         4.6667         GDM_g  16.2750  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = pd.read_csv(TRAIN_CSV)\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1004d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique samples: 1785\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>Sampling_Date</th>\n",
       "      <th>State</th>\n",
       "      <th>Species</th>\n",
       "      <th>Pre_GSHH_NDVI</th>\n",
       "      <th>Height_Ave_cm</th>\n",
       "      <th>Dry_Clover_g</th>\n",
       "      <th>Dry_Dead_g</th>\n",
       "      <th>Dry_Green_g</th>\n",
       "      <th>Dry_Total_g</th>\n",
       "      <th>GDM_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1011485656__Dry_Clover_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1011485656__Dry_Dead_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.9984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1011485656__Dry_Green_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.2751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1011485656__Dry_Total_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.2735</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1011485656__GDM_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id              image_path Sampling_Date State  \\\n",
       "0  ID1011485656__Dry_Clover_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "1    ID1011485656__Dry_Dead_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "2   ID1011485656__Dry_Green_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "3   ID1011485656__Dry_Total_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "4         ID1011485656__GDM_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "\n",
       "           Species  Pre_GSHH_NDVI  Height_Ave_cm  Dry_Clover_g  Dry_Dead_g  \\\n",
       "0  Ryegrass_Clover           0.62         4.6667           0.0         NaN   \n",
       "1  Ryegrass_Clover           0.62         4.6667           NaN     31.9984   \n",
       "2  Ryegrass_Clover           0.62         4.6667           NaN         NaN   \n",
       "3  Ryegrass_Clover           0.62         4.6667           NaN         NaN   \n",
       "4  Ryegrass_Clover           0.62         4.6667           NaN         NaN   \n",
       "\n",
       "   Dry_Green_g  Dry_Total_g   GDM_g  \n",
       "0          NaN          NaN     NaN  \n",
       "1          NaN          NaN     NaN  \n",
       "2      16.2751          NaN     NaN  \n",
       "3          NaN      48.2735     NaN  \n",
       "4          NaN          NaN  16.275  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure image_path is string\n",
    "train_raw[\"image_path\"] = train_raw[\"image_path\"].astype(str)\n",
    "\n",
    "# We'll build a per-sample table:\n",
    "# - take the first image_path found for each sample_id (CSV may duplicate)\n",
    "# - gather tabular features from the first row for that sample (Sampling_Date, State, Species, Pre_GSHH_NDVI, Height_Ave_cm)\n",
    "# - pivot target values into columns (one column per target_name)\n",
    "\n",
    "# pivot targets\n",
    "targets_pivot = train_raw.pivot_table(\n",
    "    index=\"sample_id\",\n",
    "    columns=\"target_name\",\n",
    "    values=\"target\",\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# Get one row per sample for other columns (image path and metadata) by taking first occurrence\n",
    "meta_cols = [\"image_path\", \"Sampling_Date\", \"State\", \"Species\", \"Pre_GSHH_NDVI\", \"Height_Ave_cm\"]\n",
    "meta = train_raw.groupby(\"sample_id\").first()[meta_cols]\n",
    "\n",
    "# join\n",
    "train_samples = meta.join(targets_pivot)\n",
    "train_samples = train_samples.reset_index()\n",
    "print(\"Number of unique samples:\", len(train_samples))\n",
    "train_samples.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705cf9c",
   "metadata": {},
   "source": [
    "4. Preprocess tabular features (date cyclic, one-hot, scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909cecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular shape: (1785, 23)\n"
     ]
    }
   ],
   "source": [
    "# Date cyclic encoding (day of year -> sin/cos)\n",
    "def add_date_features(df, date_col=\"Sampling_Date\"):\n",
    "    # parse dates safely:\n",
    "    dates = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    dayofyear = dates.dt.dayofyear.fillna(1).astype(int)  # default to 1 if missing\n",
    "    df[\"date_sin\"] = np.sin(2 * np.pi * dayofyear / 365.25)\n",
    "    df[\"date_cos\"] = np.cos(2 * np.pi * dayofyear / 365.25)\n",
    "    return df\n",
    "\n",
    "train_samples = add_date_features(train_samples)\n",
    "\n",
    "# Fit OneHotEncoder on State and Species (train only)\n",
    "cat_cols = [\"State\", \"Species\"]\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "enc.fit(train_samples[cat_cols].fillna(\"\"))\n",
    "\n",
    "# Numeric features to scale\n",
    "num_cols = [\"Pre_GSHH_NDVI\", \"Height_Ave_cm\"]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_samples[num_cols].fillna(0).values)\n",
    "\n",
    "# Build tabular matrix for train samples (order matters)\n",
    "def build_tabular_matrix(df, enc, scaler):\n",
    "    # date sin/cos\n",
    "    date_part = df[[\"date_sin\", \"date_cos\"]].values.astype(float)\n",
    "    # numeric scaled\n",
    "    num_part = df[num_cols].fillna(0).values.astype(float)\n",
    "    num_part_scaled = scaler.transform(num_part)\n",
    "    # categorical one-hot (enc handles unknowns if present)\n",
    "    cat_part = enc.transform(df[cat_cols].fillna(\"\"))\n",
    "    # final concat\n",
    "    return np.hstack([date_part, num_part_scaled, cat_part]).astype(np.float32)\n",
    "\n",
    "tabular_all = build_tabular_matrix(train_samples, enc, scaler)\n",
    "print(\"Tabular shape:\", tabular_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2109f7",
   "metadata": {},
   "source": [
    "5. Build multi-target arrays and mask (for missing targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b3f0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets shape: (1785, 5) Mask shape: (1785, 5)\n",
      "Observed target counts per output: [357. 357. 357. 357. 357.]\n"
     ]
    }
   ],
   "source": [
    "# Create target matrix with ordering given by TARGET_NAMES\n",
    "def build_targets_and_mask(df, target_names):\n",
    "    n = len(df)\n",
    "    m = len(target_names)\n",
    "    targets = np.zeros((n, m), dtype=np.float32)\n",
    "    mask = np.zeros((n, m), dtype=np.float32)  # 1 if target exists, 0 if missing\n",
    "    for i, row in df.iterrows():\n",
    "        for j, tname in enumerate(target_names):\n",
    "            val = row.get(tname, np.nan)\n",
    "            if pd.notna(val):\n",
    "                targets[i, j] = float(val)\n",
    "                mask[i, j] = 1.0\n",
    "    return targets, mask\n",
    "\n",
    "targets_all, mask_all = build_targets_and_mask(train_samples, TARGET_NAMES)\n",
    "print(\"Targets shape:\", targets_all.shape, \"Mask shape:\", mask_all.shape)\n",
    "# quick check counts\n",
    "print(\"Observed target counts per output:\", mask_all.sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e9fc2",
   "metadata": {},
   "source": [
    "6. Train / Validation split (split by sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ef1f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1428 Val samples: 357\n"
     ]
    }
   ],
   "source": [
    "# split samples (we'll split by index/sample not per-row)\n",
    "train_idx, val_idx = train_test_split(np.arange(len(train_samples)), test_size=0.2, random_state=42)\n",
    "\n",
    "# prepare arrays\n",
    "X_tab_train = tabular_all[train_idx]\n",
    "X_tab_val = tabular_all[val_idx]\n",
    "y_train = targets_all[train_idx]\n",
    "y_val = targets_all[val_idx]\n",
    "mask_train = mask_all[train_idx]\n",
    "mask_val = mask_all[val_idx]\n",
    "df_train = train_samples.iloc[train_idx].reset_index(drop=True)\n",
    "df_val = train_samples.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"Train samples:\", len(df_train), \"Val samples:\", len(df_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb22b7",
   "metadata": {},
   "source": [
    "7. Dataset classes (train/val & test) — robust path handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad9bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTargetPastureDataset(Dataset):\n",
    "    def __init__(self, df, tabular_array, targets_array=None, mask_array=None, transform=None, img_root=None):\n",
    "        \"\"\"\n",
    "        df: DataFrame with sample rows (must include image_path)\n",
    "        tabular_array: numpy array aligned with df rows\n",
    "        targets_array: numpy array (N, M) or None for test\n",
    "        mask_array: numpy array (N, M) same shape as targets (1 where present)\n",
    "        transform: image transform\n",
    "        img_root: pathlib.Path root folder to prepend IF image_path is relative missing folder\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tab = torch.tensor(tabular_array, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets_array, dtype=torch.float32) if targets_array is not None else None\n",
    "        self.mask = torch.tensor(mask_array, dtype=torch.float32) if mask_array is not None else None\n",
    "        self.transform = transform\n",
    "        self.img_root = pathlib.Path(img_root) if img_root is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _resolve_path(self, raw_path):\n",
    "        # raw_path may already include train/ or be just filename.\n",
    "        p = pathlib.Path(raw_path)\n",
    "        if p.exists():\n",
    "            return p\n",
    "        # try with img_root prepended if provided\n",
    "        if self.img_root is not None:\n",
    "            p2 = self.img_root / p\n",
    "            if p2.exists():\n",
    "                return p2\n",
    "        # try filename only in img_root\n",
    "        if self.img_root is not None and \"/\" in str(raw_path):\n",
    "            # try last part\n",
    "            last = pathlib.Path(raw_path).name\n",
    "            p3 = self.img_root / last\n",
    "            if p3.exists():\n",
    "                return p3\n",
    "        # fallback: return original path (will raise later)\n",
    "        return p\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self._resolve_path(row[\"image_path\"])\n",
    "        if not img_path.exists():\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        img = Image.open(str(img_path)).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        tab = self.tab[idx]\n",
    "        if self.targets is not None:\n",
    "            y = self.targets[idx]\n",
    "            m = self.mask[idx]\n",
    "            return img, tab, y, m\n",
    "        else:\n",
    "            # test\n",
    "            return img, tab, row[\"sample_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23426e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestPastureDataset(Dataset):\n",
    "    def __init__(self, df, tabular_array, transform=None, img_root=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tab = torch.tensor(tabular_array, dtype=torch.float32)\n",
    "        self.transform = transform\n",
    "        self.img_root = pathlib.Path(img_root) if img_root is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _resolve_path(self, raw_path):\n",
    "        p = pathlib.Path(raw_path)\n",
    "        if p.exists():\n",
    "            return p\n",
    "        if self.img_root is not None:\n",
    "            p2 = self.img_root / p\n",
    "            if p2.exists():\n",
    "                return p2\n",
    "            last = pathlib.Path(raw_path).name\n",
    "            p3 = self.img_root / last\n",
    "            if p3.exists():\n",
    "                return p3\n",
    "        return p\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self._resolve_path(row[\"image_path\"])\n",
    "        if not img_path.exists():\n",
    "            raise FileNotFoundError(f\"Test image not found: {img_path}\")\n",
    "        img = Image.open(str(img_path)).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        tab = self.tab[idx]\n",
    "        return img, tab, row[\"sample_id\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d059ae",
   "metadata": {},
   "source": [
    "8. Transforms & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea43eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 45 Val batches: 12\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.1,\n",
    "        contrast=0.1,\n",
    "        saturation=0.05,\n",
    "        hue=0.02\n",
    "    ),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "train_dataset = MultiTargetPastureDataset(df_train, X_tab_train, y_train, mask_train, transform=train_transform, img_root=TRAIN_IMG_ROOT)\n",
    "val_dataset   = MultiTargetPastureDataset(df_val,   X_tab_val,   y_val,   mask_val,   transform=val_transform,   img_root=TRAIN_IMG_ROOT)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader), \"Val batches:\", len(val_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93dac1a",
   "metadata": {},
   "source": [
    "9. Model: EfficientNet backbone + tabular MLP + fusion head (outputs 5 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe6aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionMultiOutputModel(nn.Module):\n",
    "    def __init__(self, tabular_dim, num_outputs=len(TARGET_NAMES), backbone_name=\"efficientnet_b0\", fusion_hidden=128):\n",
    "        super().__init__()\n",
    "        if backbone_name == \"efficientnet_b0\":\n",
    "            # torchvision EfficientNetB0\n",
    "            self.backbone = models.efficientnet_b0(weights=None)\n",
    "            # remove classifier\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            image_feat_dim = 1280\n",
    "        elif backbone_name == \"resnet18\":\n",
    "            self.backbone = models.resnet18(weights=None)  # train from scratch\n",
    "            self.backbone.fc = nn.Identity()  # remove classifier\n",
    "            image_feat_dim = 512\n",
    "\n",
    "        # tabular MLP\n",
    "        self.tab_mlp = nn.Sequential(\n",
    "            nn.Linear(tabular_dim, max(32, tabular_dim*2)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(max(32, tabular_dim*2)),\n",
    "            nn.Linear(max(32, tabular_dim*2), 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # fusion head\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(image_feat_dim + 64, fusion_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(fusion_hidden),\n",
    "            nn.Dropout(0.4), # changed to a stronger dropout\n",
    "            nn.Linear(fusion_hidden, fusion_hidden//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_hidden//2, num_outputs)  # multi-output\n",
    "        )\n",
    "\n",
    "    def forward(self, img, tab):\n",
    "        img_feat = self.backbone(img)\n",
    "        tab_feat = self.tab_mlp(tab)\n",
    "        x = torch.cat([img_feat, tab_feat], dim=1)\n",
    "        out = self.fusion(x)\n",
    "        return out\n",
    "\n",
    "# instantiate\n",
    "tab_dim = X_tab_train.shape[1]\n",
    "model = FusionMultiOutputModel(tabular_dim=tab_dim).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd01e2",
   "metadata": {},
   "source": [
    "10. Loss that respects mask (MAE only where mask==1) + optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35294b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mae_loss(preds, targets, mask):\n",
    "    \"\"\"\n",
    "    preds: (B, M)\n",
    "    targets: (B, M)\n",
    "    mask: (B, M)  -> 1 where target exists, 0 where missing\n",
    "    returns average MAE over observed entries\n",
    "    \"\"\"\n",
    "    diff = torch.abs(preds - targets) * mask\n",
    "    # sum diffs and divide by number of observed elements (avoid zero divide)\n",
    "    denom = mask.sum()\n",
    "    if denom.item() == 0:\n",
    "        return torch.tensor(0.0, device=preds.device)\n",
    "    return diff.sum() / denom\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c50d99",
   "metadata": {},
   "source": [
    "11. Training & validation loop (saves best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b7836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1 train:   0%|          | 0/45 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "best_fold_models = []  # store best model per fold\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=KFOLD_RANDOM_STATE)\n",
    "all_fold_val_maes = []\n",
    "\n",
    "                \"best_fold_models = []  # will store best model (single split)\",\n",
    "                \"all_fold_val_maes = []\",\n",
    "                \"\",\n",
    "                \"# Use a single random train/val split instead of KFold\",\n",
    "                \"idxs = np.arange(len(train_samples))\",\n",
    "                \"train_idx, val_idx = train_test_split(idxs, test_size=1.0/N_SPLITS, random_state=KFOLD_RANDOM_STATE)\",\n",
    "                \"print(f\\\"Single split: train={len(train_idx)} val={len(val_idx)}\\\")\",\n",
    "                \"\",\n",
    "                \"# prepare arrays (same naming as fold version to minimize downstream changes)\",\n",
    "                \"X_tab_train_fold = tabular_all[train_idx]\",\n",
    "                \"X_tab_val_fold = tabular_all[val_idx]\",\n",
    "                \"y_train_fold = targets_all[train_idx]\",\n",
    "                \"y_val_fold = targets_all[val_idx]\",\n",
    "                \"mask_train_fold = mask_all[train_idx]\",\n",
    "                \"mask_val_fold = mask_all[val_idx]\",\n",
    "                \"\",\n",
    "                \"df_train_fold = train_samples.iloc[train_idx].reset_index(drop=True)\",\n",
    "                \"df_val_fold = train_samples.iloc[val_idx].reset_index(drop=True)\",\n",
    "                \"\",\n",
    "                \"# datasets and loaders\",\n",
    "                \"train_dataset = MultiTargetPastureDataset(df_train_fold, X_tab_train_fold, y_train_fold, mask_train_fold,\",\n",
    "                \"                                          transform=train_transform, img_root=TRAIN_IMG_ROOT)\",\n",
    "                \"val_dataset   = MultiTargetPastureDataset(df_val_fold, X_tab_val_fold, y_val_fold, mask_val_fold,\",\n",
    "                \"                                          transform=val_transform, img_root=TRAIN_IMG_ROOT)\",\n",
    "                \"\",\n",
    "                \"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, pin_memory=PIN_MEMORY)\",\n",
    "                \"val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=PIN_MEMORY)\",\n",
    "                \"\",\n",
    "                \"# instantiate model\",\n",
    "                \"model_fold = FusionMultiOutputModel(tabular_dim=tab_dim).to(DEVICE)\",\n",
    "                \"optimizer = torch.optim.AdamW(model_fold.parameters(), lr=LR, weight_decay=1e-5)\",\n",
    "                \"\",\n",
    "                \"best_val = float(\\\"inf\\\")\",\n",
    "                \"best_state_dict = None\",\n",
    "                \"\",\n",
    "                \"for epoch in range(1, EPOCHS+1):\",\n",
    "                \"    # --- train ---\",\n",
    "                \"    model_fold.train()\",\n",
    "                \"    train_loss = 0.0\",\n",
    "                \"    seen = 0\",\n",
    "                \"    for imgs, tabs, ys, masks in tqdm(train_loader, desc=f\\\"Epoch {epoch} train\\\"):\",\n",
    "                \"        imgs = imgs.to(DEVICE)\",\n",
    "                \"        tabs = tabs.to(DEVICE)\",\n",
    "                \"        ys = ys.to(DEVICE)\",\n",
    "                \"        masks = masks.to(DEVICE)\",\n",
    "                \"\",\n",
    "                \"        preds = model_fold(imgs, tabs)\",\n",
    "                \"        loss = masked_mae_loss(preds, ys, masks)\",\n",
    "                \"\",\n",
    "                \"        optimizer.zero_grad()\",\n",
    "                \"        loss.backward()\",\n",
    "                \"        optimizer.step()\",\n",
    "                \"\",\n",
    "                \"        batch_n = imgs.size(0)\",\n",
    "                \"        train_loss += loss.item() * batch_n\",\n",
    "                \"        seen += batch_n\",\n",
    "                \"\",\n",
    "                \"    train_loss_epoch = train_loss / max(seen,1)\",\n",
    "                \"\",\n",
    "                \"    # --- validate ---\",\n",
    "                \"    model_fold.eval()\",\n",
    "                \"    val_loss = 0.0\",\n",
    "                \"    seen_val = 0\",\n",
    "                \"    with torch.no_grad():\",\n",
    "                \"        for imgs, tabs, ys, masks in tqdm(val_loader, desc=f\\\"Epoch {epoch} val\\\"):\",\n",
    "                \"            imgs = imgs.to(DEVICE)\",\n",
    "                \"            tabs = tabs.to(DEVICE)\",\n",
    "                \"            ys = ys.to(DEVICE)\",\n",
    "                \"            masks = masks.to(DEVICE)\",\n",
    "                \"\",\n",
    "                \"            preds = model_fold(imgs, tabs)\",\n",
    "                \"            loss = masked_mae_loss(preds, ys, masks)\",\n",
    "                \"            batch_n = imgs.size(0)\",\n",
    "                \"            val_loss += loss.item() * batch_n\",\n",
    "                \"            seen_val += batch_n\",\n",
    "                \"\",\n",
    "                \"    val_loss_epoch = val_loss / max(seen_val,1)\",\n",
    "                \"    print(f\\\"Epoch {epoch}: Train MAE={train_loss_epoch:.4f}  Val MAE={val_loss_epoch:.4f}\\\")\",\n",
    "                \"\",\n",
    "                \"    if val_loss_epoch < best_val:\",\n",
    "                \"        best_val = val_loss_epoch\",\n",
    "                \"        best_state_dict = model_fold.state_dict()\",\n",
    "                \"        epochs_no_improve = 0\",\n",
    "                \"        print(f\\\"New best model! Val MAE={best_val:.4f}\\\")\",\n",
    "                \"    else:\",\n",
    "                \"        epochs_no_improve += 1\",\n",
    "                \"        if epochs_no_improve >= EARLY_STOP_PATIENCE:\",\n",
    "                \"            print(f\\\"Early stopping at epoch {epoch} (no improvement in {EARLY_STOP_PATIENCE} epochs)\\\")\",\n",
    "                \"            break  \",\n",
    "                \"\",\n",
    "                \"all_fold_val_maes.append(best_val)\",\n",
    "                \"best_fold_models.append(best_state_dict)\",\n",
    "                \"\",\n",
    "                \"print(\\\"\\nValidation MAE:\\\", all_fold_val_maes)\",\n",
    "                \"print(\\\"Mean MAE:\\\", np.mean(all_fold_val_maes))\"\n",
    "    \n",
    "    all_fold_val_maes.append(best_val)\n",
    "    best_fold_models.append(best_state_dict)\n",
    "\n",
    "print(\"\\nAll fold validation MAEs:\", all_fold_val_maes)\n",
    "print(\"Mean MAE across folds:\", np.mean(all_fold_val_maes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c05fb",
   "metadata": {},
   "source": [
    "12. Prepare test samples (build per-sample table) and tabular zeros fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ee72c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ced0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test tabular shape: (10, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1001187975__Dry_Clover_g</td>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1001187975__Dry_Dead_g</td>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1001187975__Dry_Green_g</td>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1001187975__Dry_Total_g</td>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1001187975__GDM_g</td>\n",
       "      <td>test/ID1001187975.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id             image_path\n",
       "0  ID1001187975__Dry_Clover_g  test/ID1001187975.jpg\n",
       "1    ID1001187975__Dry_Dead_g  test/ID1001187975.jpg\n",
       "2   ID1001187975__Dry_Green_g  test/ID1001187975.jpg\n",
       "3   ID1001187975__Dry_Total_g  test/ID1001187975.jpg\n",
       "4         ID1001187975__GDM_g  test/ID1001187975.jpg"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw = pd.read_csv(TEST_CSV)\n",
    "test_raw[\"image_path\"] = test_raw[\"image_path\"].astype(str)\n",
    "\n",
    "# Test has one row per sample_id x target_name (like the sample submission)\n",
    "# We need unique sample rows (one image per sample_id)\n",
    "test_samples = test_raw.groupby(\"sample_id\").first().reset_index()[[\"sample_id\", \"image_path\"]]\n",
    "\n",
    "# Build tabular for test: test CSV lacks metadata, so we create zeros for tabular inputs\n",
    "# But we must match the encoder's feature length: date(2) + num(2) + cat(len)\n",
    "date_placeholders = np.zeros((len(test_samples), 2), dtype=np.float32)\n",
    "num_placeholders = np.zeros((len(test_samples), len(num_cols)), dtype=np.float32)\n",
    "# scaled num => zeros (same transform)\n",
    "num_scaled_test = (num_placeholders - scaler.mean_) / scaler.scale_\n",
    "# categorical => zeros (no categories known)\n",
    "cat_dim = sum(len(cats) for cats in enc.categories_)\n",
    "cat_zeros = np.zeros((len(test_samples), cat_dim), dtype=np.float32)\n",
    "\n",
    "tabular_test = np.hstack([date_placeholders, num_scaled_test, cat_zeros]).astype(np.float32)\n",
    "print(\"Test tabular shape:\", tabular_test.shape)\n",
    "test_samples.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e9afe",
   "metadata": {},
   "source": [
    "13. Test dataloader & inference (produce predictions per sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504cf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test infer: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = TestPastureDataset(test_samples, tabular_test, transform=val_transform, img_root=TEST_IMG_ROOT)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Initialize predictions storage: sample_id -> list of predicted vectors\n",
    "pred_map_folds = {sid: [] for sid in test_samples[\"sample_id\"].values}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for fold_idx, best_state_dict in enumerate(best_fold_models, 1):\n",
    "        print(f\"Running inference with fold {fold_idx} best model\")\n",
    "        model.load_state_dict(best_state_dict)\n",
    "        model.eval()\n",
    "        \n",
    "        for imgs, tabs, sample_ids in tqdm(test_loader, desc=f\"Fold {fold_idx} inference\"):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            tabs = tabs.to(DEVICE)\n",
    "            preds = model(imgs, tabs)  # (B, M)\n",
    "            preds_np = preds.cpu().numpy()  # (B, M)\n",
    "            \n",
    "            for sid, pred_vals in zip(sample_ids, preds_np):\n",
    "                pred_map_folds[sid].append(pred_vals)\n",
    "\n",
    "# Average predictions across folds\n",
    "pred_map_avg = {sid: np.mean(pred_list, axis=0) for sid, pred_list in pred_map_folds.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87862447",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '\\kaggle\\working'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m submission_df = pd.DataFrame({\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msample_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(pred_map.keys()),\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(pred_map.values())\n\u001b[32m      4\u001b[39m })\n\u001b[32m      6\u001b[39m submission_df[\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m] = submission_df[\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m].clip(lower=\u001b[32m0\u001b[39m).round(\u001b[32m2\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43msubmission_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKAGGLE_OUTPUT_DIR\u001b[49m\u001b[43m+\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msubmission.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved submission.csv with\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(submission_df), \u001b[33m\"\u001b[39m\u001b[33mrows\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m submission_df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Robert\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Robert\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:3989\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3978\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3980\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3981\u001b[39m     frame=df,\n\u001b[32m   3982\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3986\u001b[39m     decimal=decimal,\n\u001b[32m   3987\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3989\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4006\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Robert\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Robert\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Robert\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Robert\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '\\kaggle\\working'"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    \"sample_id\": list(pred_map_avg.keys()),\n",
    "    \"target\": list(pred_map_avg.values())\n",
    "})\n",
    "\n",
    "submission_df[\"target\"] = submission_df[\"target\"].clip(lower=0).round(2)\n",
    "submission_df.to_csv(KAGGLE_OUTPUT_DIR+\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Saved submission.csv with\", len(submission_df), \"rows\")\n",
    "submission_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
