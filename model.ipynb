{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9531125f",
   "metadata": {},
   "source": [
    "Install + Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbc72b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision pandas scikit-learn --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500607bb",
   "metadata": {},
   "source": [
    "Load CSV + Preprocess Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a13569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Convert date to cyclic features\n",
    "df[\"date\"] = pd.to_datetime(df[\"Sampling_Date\"])\n",
    "df[\"dayofyear\"] = df[\"date\"].dt.dayofyear\n",
    "df[\"date_sin\"] = np.sin(2 * np.pi * df[\"dayofyear\"] / 365.25)\n",
    "df[\"date_cos\"] = np.cos(2 * np.pi * df[\"dayofyear\"] / 365.25)\n",
    "\n",
    "# Categorical encoding\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "cat_features = enc.fit_transform(df[[\"State\", \"Species\"]])\n",
    "\n",
    "# Numeric features\n",
    "num_features = df[[\"Pre_GSHH_NDVI\", \"Height_Ave_cm\"]].values\n",
    "scaler = StandardScaler()\n",
    "num_scaled = scaler.fit_transform(num_features)\n",
    "\n",
    "# Final tabular input\n",
    "tabular = np.hstack([df[\"date_sin\"].values.reshape(-1,1),\n",
    "                     df[\"date_cos\"].values.reshape(-1,1),\n",
    "                     num_scaled,\n",
    "                     cat_features])\n",
    "\n",
    "targets = df[\"target\"].values\n",
    "\n",
    "train_df, val_df, tab_train, tab_val, y_train, y_val = train_test_split(\n",
    "    df, tabular, targets, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c4014",
   "metadata": {},
   "source": [
    "Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5fabe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PastureDataset(Dataset):\n",
    "    def __init__(self, df, tabular_data, targets, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tabular = torch.tensor(tabular_data, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = pathlib.Path(row[\"image_path\"])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        tab = self.tabular[idx]\n",
    "        y = self.targets[idx]\n",
    "        return img, tab, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982c396",
   "metadata": {},
   "source": [
    "Transforms + Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e2f002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = PastureDataset(train_df, tab_train, y_train, transform)\n",
    "val_dataset   = PastureDataset(val_df,   tab_val,   y_val,   transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47777ac9",
   "metadata": {},
   "source": [
    "Fusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ba6d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, tab_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = models.efficientnet_b0(pretrained=True)\n",
    "        self.cnn.classifier = nn.Identity()\n",
    "        image_feat_dim = 1280\n",
    "\n",
    "        self.tab_mlp = nn.Sequential(\n",
    "            nn.Linear(tab_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(image_feat_dim + 64, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, tab):\n",
    "        img_feat = self.cnn(img)\n",
    "        tab_feat = self.tab_mlp(tab)\n",
    "        fused = torch.cat([img_feat, tab_feat], dim=1)\n",
    "        return self.fusion(fused)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c932a1",
   "metadata": {},
   "source": [
    "Training Loop + Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4fdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robert\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Robert\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 90/90 [03:45<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train: 21.8987 | Val: 21.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [03:33<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train: 18.7369 | Val: 18.7507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [03:24<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train: 17.4404 | Val: 18.9351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [03:40<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train: 17.2124 | Val: 18.9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [03:34<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train: 16.6183 | Val: 19.3267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [03:30<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = FusionModel(tab_dim=tab_train.shape[1]).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()  # MAE\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for img, tab, y in loader:\n",
    "            img, tab, y = img.to(device), tab.to(device), y.to(device).unsqueeze(1)\n",
    "            preds = model(img, tab)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img, tab, y in tqdm(train_loader):\n",
    "        img, tab, y = img.to(device), tab.to(device), y.to(device).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(img, tab)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print(f\"Epoch {epoch+1} | Train: {total_loss/len(train_loader):.4f} | Val: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ee244",
   "metadata": {},
   "source": [
    "Save trained model weights (run after training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b269d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights and optionally the whole checkpoint (so you can reload later)\n",
    "checkpoint_path = \"fusion_model_final.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'enc_categories': enc.categories_,    # saves categories for later use\n",
    "    'scaler_mean': scaler.mean_,\n",
    "    'scaler_scale': scaler.scale_,\n",
    "    'tabular_dim': tab_train.shape[1]\n",
    "}, checkpoint_path)\n",
    "print(\"Saved checkpoint to\", checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc24fb96",
   "metadata": {},
   "source": [
    "Prepare test.csv (encoding must match train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test.csv and compute same tabular representation\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# If Sampling_Date might be missing or formatted differently, handle safely\n",
    "test_df[\"date\"] = pd.to_datetime(test_df[\"Sampling_Date\"], errors='coerce')\n",
    "test_df[\"dayofyear\"] = test_df[\"date\"].dt.dayofyear.fillna(1).astype(int)  # default day 1 if missing\n",
    "test_df[\"date_sin\"] = np.sin(2 * np.pi * test_df[\"dayofyear\"] / 365.25)\n",
    "test_df[\"date_cos\"] = np.cos(2 * np.pi * test_df[\"dayofyear\"] / 365.25)\n",
    "\n",
    "# Ensure categorical columns exist (State, Species)\n",
    "# If there are unseen categories in test, OneHotEncoder will fail — handle by mapping unseen to zero vector.\n",
    "def safe_onehot(encoder, df, cols):\n",
    "    # Build manual one-hot using encoder.categories_\n",
    "    arrays = []\n",
    "    for i, col in enumerate(cols):\n",
    "        cats = encoder.categories_[i]\n",
    "        vals = df[col].astype(str).values\n",
    "        onehot = np.zeros((len(vals), len(cats)), dtype=float)\n",
    "        for j, v in enumerate(vals):\n",
    "            # find index if present\n",
    "            try:\n",
    "                idx = np.where(cats == v)[0][0]\n",
    "                onehot[j, idx] = 1.0\n",
    "            except IndexError:\n",
    "                # unseen category -> all zeros (you might prefer to map to 'unknown' if present)\n",
    "                pass\n",
    "        arrays.append(onehot)\n",
    "    return np.hstack(arrays)\n",
    "\n",
    "cat_cols = [\"State\", \"Species\"]\n",
    "if set(cat_cols).issubset(test_df.columns):\n",
    "    cat_features_test = safe_onehot(enc, test_df, cat_cols)\n",
    "else:\n",
    "    # if missing, create empty\n",
    "    cat_features_test = np.zeros((len(test_df), sum(len(c) for c in enc.categories_)), dtype=float)\n",
    "\n",
    "# Numeric features (must match the order used in training)\n",
    "num_feats_test = test_df[[\"Pre_GSHH_NDVI\", \"Height_Ave_cm\"]].fillna(0).values.astype(float)\n",
    "# Apply training scaler: use stored mean/scale (scaler object should exist)\n",
    "num_scaled_test = (num_feats_test - scaler.mean_) / scaler.scale_\n",
    "\n",
    "tabular_test = np.hstack([\n",
    "    test_df[\"date_sin\"].values.reshape(-1,1),\n",
    "    test_df[\"date_cos\"].values.reshape(-1,1),\n",
    "    num_scaled_test,\n",
    "    cat_features_test\n",
    "])\n",
    "\n",
    "print(\"Prepared tabular_test shape:\", tabular_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4864dde",
   "metadata": {},
   "source": [
    "Test Dataset + DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c3d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestPastureDataset(Dataset):\n",
    "    def __init__(self, df, tabular_data, img_folder=\"test\", transform=None, img_col=\"image_path\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tabular = torch.tensor(tabular_data, dtype=torch.float32)\n",
    "        self.transform = transform\n",
    "        self.img_folder = img_folder\n",
    "        self.img_col = img_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_folder, row[self.img_col])\n",
    "        # fallback: if path is absolute in CSV, use it directly\n",
    "        if not os.path.exists(img_path) and os.path.exists(row[self.img_col]):\n",
    "            img_path = row[self.img_col]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        tab = self.tabular[idx]\n",
    "        return img, tab, row[\"sample_id\"]\n",
    "\n",
    "test_dataset = TestPastureDataset(test_df, tabular_test, img_folder=\"test\", transform=transform, img_col=\"image_path\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "print(\"Test samples:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fba551",
   "metadata": {},
   "source": [
    "Run Inference & Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8758d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "preds = []\n",
    "sample_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, tabs, ids in tqdm(test_loader, desc=\"Infer\"):\n",
    "        imgs = imgs.to(device)\n",
    "        tabs = tabs.to(device)\n",
    "        out = model(imgs, tabs)  # shape (B,1) or (B,)\n",
    "        out = out.squeeze(1).cpu().numpy()\n",
    "        preds.extend(out.tolist())\n",
    "        sample_ids.extend(list(ids))\n",
    "\n",
    "# Build DataFrame and save\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": sample_ids,\n",
    "    \"prediction\": preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved submission.csv with\", len(submission), \"rows\")\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
