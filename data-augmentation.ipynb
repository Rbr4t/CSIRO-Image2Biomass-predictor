{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T18:54:27.274406Z","iopub.execute_input":"2025-12-03T18:54:27.274788Z","iopub.status.idle":"2025-12-03T18:54:27.279744Z","shell.execute_reply.started":"2025-12-03T18:54:27.274766Z","shell.execute_reply":"2025-12-03T18:54:27.278714Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Imports","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\n\nprint(\"Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T18:54:31.518753Z","iopub.execute_input":"2025-12-03T18:54:31.519392Z","iopub.status.idle":"2025-12-03T18:54:31.523888Z","shell.execute_reply.started":"2025-12-03T18:54:31.519362Z","shell.execute_reply":"2025-12-03T18:54:31.523097Z"}},"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Input files\nCSV_IN = Path(\"/kaggle/input/csiro-biomass/train.csv\")\nIMG_ROOT = Path(\"/kaggle/input/csiro-biomass\")  # image paths in CSV are like 'train/ID...jpg'\n\n# Output\nOUT_IMG_DIR = Path(\"/kaggle/working/train_aug\")\nOUT_CSV = Path(\"/kaggle/working/train_augmented.csv\")\n\n# We create augmentations:\nCREATE_ROT180 = True\nCREATE_FLIP_X = True\nCREATE_FLIP_Y = True\n\nOUT_IMG_DIR.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T18:54:36.067072Z","iopub.execute_input":"2025-12-03T18:54:36.067393Z","iopub.status.idle":"2025-12-03T18:54:36.072904Z","shell.execute_reply.started":"2025-12-03T18:54:36.067358Z","shell.execute_reply":"2025-12-03T18:54:36.072119Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# helper function to create filenames\ndef make_augmented_filename(orig_filename: str, suffix: str):\n    p = Path(orig_filename)\n    return f\"{p.stem}{suffix}{p.suffix}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T18:54:39.177130Z","iopub.execute_input":"2025-12-03T18:54:39.177859Z","iopub.status.idle":"2025-12-03T18:54:39.181649Z","shell.execute_reply.started":"2025-12-03T18:54:39.177834Z","shell.execute_reply":"2025-12-03T18:54:39.180834Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"df = pd.read_csv(CSV_IN)\n\n#build a dataframe of augmented rows\n\naug_rows = []\nunique_image_paths = df['image_path'].unique()\ncreated_images = {}\n\nfor img_rel in tqdm(unique_image_paths, desc=\"Augmenting images\"):\n    src_img_path = IMG_ROOT / img_rel\n    img = Image.open(src_img_path).convert(\"RGB\")\n\n    created_images[img_rel] = []\n    basename = Path(img_rel).name\n\n    # 1) rotate 180\n    if CREATE_ROT180:\n        out_name = make_augmented_filename(basename, \"_rot180\")\n        out_path = OUT_IMG_DIR / out_name\n        img_rot = img.rotate(180)\n        img_rot.save(out_path, quality=95)\n        created_images[img_rel].append(out_name)\n\n    # 2) flip x axis\n    if CREATE_FLIP_X:\n        out_name = make_augmented_filename(basename, \"_flipx\")\n        out_path = OUT_IMG_DIR / out_name\n        img_fx = img.transpose(Image.FLIP_TOP_BOTTOM)\n        img_fx.save(out_path, quality=95)\n        created_images[img_rel].append(out_name)\n\n    # 3) flip y axis\n    if CREATE_FLIP_Y:\n        out_name = make_augmented_filename(basename, \"_flipy\")\n        out_path = OUT_IMG_DIR / out_name\n        img_fy = img.transpose(Image.FLIP_LEFT_RIGHT)\n        img_fy.save(out_path, quality=95)\n        created_images[img_rel].append(out_name)\n\n# Now duplicate CSV rows for each augmented image\nfor _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building augmented CSV rows\"):\n    img_rel = row['image_path']\n    for out_name in created_images[img_rel]:\n        new_row = row.copy()\n        # append suffix to sample_id to keep names unique\n        orig_stem = Path(img_rel).stem\n        aug_stem = Path(out_name).stem\n        suffix = aug_stem.replace(orig_stem, \"\")\n        new_row['sample_id'] = row['sample_id'] + suffix\n        # Set image_path to point to where we wrote it. Use a path relative to the new CSV consumer.\n        # I choose 'train_aug/<filename>' as the new relative path (you can adjust).\n        new_row['image_path'] = str(Path(\"train_aug\") / out_name)\n        aug_rows.append(new_row)\n\n# Create new dataframe and merge with original\ndf_aug = pd.DataFrame(aug_rows)\nfinal_df = pd.concat([df, df_aug], ignore_index=True)\n\nprint(f\"Original rows: {len(df):,}\")\nprint(f\"Augmented rows added: {len(df_aug):,}\")\nprint(f\"Final rows: {len(final_df):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T18:56:30.313842Z","iopub.execute_input":"2025-12-03T18:56:30.314476Z","iopub.status.idle":"2025-12-03T18:57:14.459103Z","shell.execute_reply.started":"2025-12-03T18:56:30.314445Z","shell.execute_reply":"2025-12-03T18:57:14.458135Z"}},"outputs":[{"name":"stderr","text":"Augmenting images: 100%|██████████| 357/357 [00:43<00:00,  8.24it/s]\nBuilding augmented CSV rows: 100%|██████████| 1785/1785 [00:00<00:00, 3962.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Original rows: 1,785\nAugmented rows added: 5,355\nFinal rows: 7,140\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# save new CSV and show a small sample\nfinal_df.to_csv(OUT_CSV, index=False)\nprint(f\"Augmented images saved to: {OUT_IMG_DIR}\")\nprint(f\"Augmented CSV saved to: {OUT_CSV}\")\n\ndisplay(df_aug.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T18:57:23.200122Z","iopub.execute_input":"2025-12-03T18:57:23.200983Z","iopub.status.idle":"2025-12-03T18:57:23.278015Z","shell.execute_reply.started":"2025-12-03T18:57:23.200955Z","shell.execute_reply":"2025-12-03T18:57:23.277255Z"}},"outputs":[{"name":"stdout","text":"Augmented images saved to: /kaggle/working/train_aug\nAugmented CSV saved to: /kaggle/working/train_augmented.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                           sample_id                         image_path  \\\n0  ID1011485656__Dry_Clover_g_rot180  train_aug/ID1011485656_rot180.jpg   \n0   ID1011485656__Dry_Clover_g_flipx   train_aug/ID1011485656_flipx.jpg   \n0   ID1011485656__Dry_Clover_g_flipy   train_aug/ID1011485656_flipy.jpg   \n1    ID1011485656__Dry_Dead_g_rot180  train_aug/ID1011485656_rot180.jpg   \n1     ID1011485656__Dry_Dead_g_flipx   train_aug/ID1011485656_flipx.jpg   \n\n  Sampling_Date State          Species  Pre_GSHH_NDVI  Height_Ave_cm  \\\n0      2015/9/4   Tas  Ryegrass_Clover           0.62         4.6667   \n0      2015/9/4   Tas  Ryegrass_Clover           0.62         4.6667   \n0      2015/9/4   Tas  Ryegrass_Clover           0.62         4.6667   \n1      2015/9/4   Tas  Ryegrass_Clover           0.62         4.6667   \n1      2015/9/4   Tas  Ryegrass_Clover           0.62         4.6667   \n\n    target_name   target  \n0  Dry_Clover_g   0.0000  \n0  Dry_Clover_g   0.0000  \n0  Dry_Clover_g   0.0000  \n1    Dry_Dead_g  31.9984  \n1    Dry_Dead_g  31.9984  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>image_path</th>\n      <th>Sampling_Date</th>\n      <th>State</th>\n      <th>Species</th>\n      <th>Pre_GSHH_NDVI</th>\n      <th>Height_Ave_cm</th>\n      <th>target_name</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID1011485656__Dry_Clover_g_rot180</td>\n      <td>train_aug/ID1011485656_rot180.jpg</td>\n      <td>2015/9/4</td>\n      <td>Tas</td>\n      <td>Ryegrass_Clover</td>\n      <td>0.62</td>\n      <td>4.6667</td>\n      <td>Dry_Clover_g</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>ID1011485656__Dry_Clover_g_flipx</td>\n      <td>train_aug/ID1011485656_flipx.jpg</td>\n      <td>2015/9/4</td>\n      <td>Tas</td>\n      <td>Ryegrass_Clover</td>\n      <td>0.62</td>\n      <td>4.6667</td>\n      <td>Dry_Clover_g</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>ID1011485656__Dry_Clover_g_flipy</td>\n      <td>train_aug/ID1011485656_flipy.jpg</td>\n      <td>2015/9/4</td>\n      <td>Tas</td>\n      <td>Ryegrass_Clover</td>\n      <td>0.62</td>\n      <td>4.6667</td>\n      <td>Dry_Clover_g</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID1011485656__Dry_Dead_g_rot180</td>\n      <td>train_aug/ID1011485656_rot180.jpg</td>\n      <td>2015/9/4</td>\n      <td>Tas</td>\n      <td>Ryegrass_Clover</td>\n      <td>0.62</td>\n      <td>4.6667</td>\n      <td>Dry_Dead_g</td>\n      <td>31.9984</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID1011485656__Dry_Dead_g_flipx</td>\n      <td>train_aug/ID1011485656_flipx.jpg</td>\n      <td>2015/9/4</td>\n      <td>Tas</td>\n      <td>Ryegrass_Clover</td>\n      <td>0.62</td>\n      <td>4.6667</td>\n      <td>Dry_Dead_g</td>\n      <td>31.9984</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17}]}